# 分布式文件系统
### 一. 本地文件系统
为什么需要文件系统：
* 对数据的操作时通过内存和CPU，其对数据的存储都是暂时的，需要永久的保存
* 直接调用磁盘接口比较复杂，需要提供一个对上层应用方便的接口。
* 文件系统的读写操作主要包括：open read write close，open是将offset置0，read是从offset中读取一个size的距离，close将文件从内存中删除

文件系统作用：
* 将文件名字翻译定位到一个具体的磁盘位置，然后调用底层接口完成文件的读写功能

VFS和NFS：
* 虚拟文件系统（VFS）是物理文件系统与服务之间的一个接口层，其实是对文件系统接口进行了标准化
  * VFS-->VFAT|EXT2|EXT3|NFS-->Device Driver-->Driver
* 网络文件系统（NFS）无需进行分布式环境的定位，通过网络传输实现数据访问，并以mount到本地的形式来简化访问。
  * mount "NFSserver:/home/sharefile" "/home/data/sharefile"

其他讨论：
* 关于磁盘块大小的讨论
  * 内存一般选择是512b磁盘块，文件系统基本上用4kB
* 文件系统的缓存
  * 缓存能够加速的必要条件，时间局部性与空间局部性
* 磁盘系统的优化策略
  * 磁盘的顺序读写（100MB）和随机读写(1MB)，如何进行磁盘优化


### 二. GFS(Google File System)

解决方案：大规模的x86集群，这些节点比较廉价，并且系统的模块还会出错

#### 1. 目标：
1. 容错性和可靠性
  * 组件失效认为常态时间，故通过数据块通过副本的方式保存在多个节点上（一般3个），保证数据的容错性。
2. 支持读写超大规模文件，文件超过数百条GB
  * 将被划分为大小为64MB的数据块进行存储，远大于一般的文件系统数据块大小
3. 负载均衡和扩展性
  * 依据全局动态信息，自动调整数据在不同服务器中的存放，负载均衡调整
  * 通过master节点实现负载均衡和扩展性
4. 系统简洁
  * master节点协调本地文件系统的访问流程
  * 同时提供类似传统文件系统的API接口函数，传统的增删改查，还包括**快照和记录追加**
5. 读写模式：写入一次，多次读取
  * 绝大多数文件修改都是尾部追加而不是覆盖原有数据，即对随机写入操作实际上几乎不存在。写入之后，通过对文件的操作只有顺序读。
  * 对数据追加进行了性能优化且需要保证原子性

#### 2. 架构：

  ![GFS原理图](https://github.com/computerwan/Java/blob/master/attachment/GFS%E5%8E%9F%E7%90%86%E5%9B%BE.jpg)

* Master节点管理着所有元数据，包括：名字空间、访问控制信息、文件和Chunk的映射信息、以及当前Chunk的位置信息；还管理了chunk的租期、orphaned chunks的回收，以及chunk在chunk服务器中的移动。
* 其他服务器（chunkServer）保存具体数据，包含多个chunk，出于可靠性，每个chunk会保存在多个服务器中，默认是3个；
> 即一个chunkServer包含多个chunk，一个chunk的副本保存在多个chunkServer中

#### 3. 步骤：
1. 客户端把文件名和程序指定的字节偏移，根据固定的Chunk大小，转换成文件的Chunk索引发送给Master节点
2. Master节点将相应的Chunk标识和副本的位置信息发还给客户端
3. 客户端用文件名和Chunk索引作为key**缓存**这些信息
5. 之后客户端发送请求到其中的一个副本处，一般会选择最近的。请求信息包含了Chunk的标识和字节范围。在对这个Chunk的后续读取操作中，**客户端不必再和Master节点通讯了**，除非缓存的元数据信息过期或者文件被重新打开。

####  问题1：为什么选择64MB作为一个chunk
* master服务器只需要不到65个字节就能管理一个chunk
1. 减少客户端和Master节点的通讯需求
2. 采用较大尺寸Chunk能让客户端对一个chunk进行多次操作，即可以通过与Chunk服务器保持长时间的TCP连接减少网络负荷。
3. 减少了Master服务器节点需要保存的元数据的数量，即可以把元数据都存到内存上。


#### 4. 性能，可靠性和一致性问题
性能问题：
  使用master服务器动态调整负载以及扩展服务器

可靠性问题：
* 块节点出现问题：
  * 通过master服务器的帮助，将两个副本恢复为三个副本，通过多台服务器的多个chunk并行恢复。
* Master出现问题：
  * 将元数据的变更以操作日志的形式保存到硬盘中
  * 当日志增长到一定量的时候对系统状态做一次Checkpoint（以B-树形式存储）
  * 有些:主服务器的操作也会备份到shadow服务器中；同时也可以多保存几个副本

一致性问题：GFS中是放松一致性
一致性(consistent):三个副本内容一样
明确（Defined）:文件顺序改变（A和B写入，出现A1和B2及B1和A2的组合）

||写入操作Write|追加操作append|
|---|:---:|:---:|
|顺序写入|一致且明确|一致且明确，但部分数据不一致|
|并行写入|一致，但不一定明确|一致且明确，但部分数据不一致|
|写入失败|不一致|不一致|

通过以下方式保证一致性：
> 方法：通过租约(lease)和变更顺序保证一致性，同时节省Master的管理负担

![](https://github.com/computerwan/Java/blob/master/attachment/GFS%E4%B8%80%E8%87%B4%E6%80%A7%E8%A6%81%E6%B1%82.jpg)

1. 客户机向Master节点轮询那个Chunk持有当前租期，其副本位置。如果没有，则新建。
2. Master节点返回Chunk标识和副本给客户机，客户机缓存。
3. 客户机把数据推送给所有副本。（任意顺序）
4. 所有机器收到副本，客户机发送给主Chunk，按照指定顺序执行
5. 主chunk将命令分配给所有副本
6. 副本执行好回复
7. 主chunk执行好回复

* Master节点为Chunk的一个副本建立一个租约，将该副本作为主Chunk
* 主Chunk对Chunk的所有更改操作进行序列化，所有副本都遵循这个顺序进行修改操作
* 主Chunk可以默认租约时间60s，之后可以申请延长租期，如果失去联系，则可以更换


#### 5. Master操作
* 使用名称空间管理和锁来保证执行的顺序
* 副本的位置选取保证了最大化数据可靠性和可用性，最大化网络带宽利用率
* 动态负载均衡：检查当前副本分布情况，然后移动副本以便更好利用硬盘


#### 6. 垃圾回收
  * 立即把删除操作的日志记录下来，然后并不马上回收资源。而是删除timsstamp并隐藏；Master做常规扫描的时候才会删除所有三天前的隐藏文件。
  * 其将垃圾回收操作合并到Master节点规律性的后台活动中
  * 为意外的，不可逆的删除操作提供了保障

#### 7 .其他：
   * 原子追加
   * 副本
    * 瞬间完成对一个文件或者目录树的拷贝，且不对其他操作造成任何干扰
    * 使用copy-on-write操作实现，执行时需要取消所有chunk租约

[参考1](http://www.open-open.com/lib/view/open1328763454608.html)
